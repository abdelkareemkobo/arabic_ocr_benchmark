{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "from jiwer import wer, cer\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_to_benchmark = [    \n",
    "    \"ahmedheakl/arocrbench_synthesizear\",\n",
    "    \"ahmedheakl/arocrbench_patsocr\",\n",
    "    \"ahmedheakl/arocrbench_arabicocr\",\n",
    "    \"ahmedheakl/arocrbench_hindawi\",\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c5d25af8df344e2840cda9bba195a40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.12k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e25d905fdad40cea3d9b3953a4b4e2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/56.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50812b2698de4b50ae66010127bd9e93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0d771201f1d4485bae52bbc65a5d0b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00007.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38f880260cf14dbbbecdb3aa55467c82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00007.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53b870d3f4634956885165c473c04289",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00007.safetensors:   0%|          | 0.00/4.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0565ebedf6947ff92782f79b7b96ab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00007.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b7dce48996642f49bea8db3f608afa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00007.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4ddd454f41e4a47978126241d4275ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00007.safetensors:   0%|          | 0.00/4.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4694e9591a9343ddbe3f73a91cd2f232",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00007-of-00007.safetensors:   0%|          | 0.00/3.38G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2972dc1a692462eacd0a88989849867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f901fb305164dd79261cecab3f0f0df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1776a1bd3da4ce08f50c70be4d3f7d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/4.30k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac68a34ce30542828e21fa7848d2f713",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2ca5da4366c485ea5c7bddcce6dd5d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efc59771d36146559dc74bfb8eb98015",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aa289ffb86c48e98a450686a54bd719",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/392 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcd96ed0603b47778e2316edb51909c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/613 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0f2ddea63984e739b3936e8bf9778cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.json:   0%|          | 0.00/1.05k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"MBZUAI/AIN\", torch_dtype=\"auto\", device_map=\"auto\"\n",
    ")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"MBZUAI/AIN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": \"Just return the plain text representation of this document as if you were reading it naturally. Do not hallucinate\"}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "text_prompt = processor.apply_chat_template(messages, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qwen_extract_ocr(text_prompt, image):\n",
    "    image = resize_image(image, min_factor=28, max_size=(1024, 1024))\n",
    "    inputs = processor(text=[text_prompt], images=[image], padding=True, return_tensors=\"pt\")\n",
    "    inputs = inputs.to(\"cuda\")\n",
    "    output_ids = model.generate(**inputs, max_new_tokens=1024)\n",
    "    generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]\n",
    "    output_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    return \" \".join(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_diacritics(text):\n",
    "    diacritics = [\n",
    "        '\\u0617', '\\u0618', '\\u0619', '\\u061A',\n",
    "        '\\u064B', '\\u064C', '\\u064D', '\\u064E', '\\u064F', '\\u0650',\n",
    "        '\\u0651', '\\u0652', '\\u0653', '\\u0654', '\\u0655', '\\u0656',\n",
    "        '\\u0657', '\\u0658', '\\u0659', '\\u065A', '\\u065B', '\\u065C',\n",
    "        '\\u065D', '\\u065E', '\\u065F', '\\u0670'\n",
    "    ]\n",
    "    pattern = '[' + ''.join(diacritics) + ']'\n",
    "    return re.sub(pattern, '', text)\n",
    "\n",
    "def remove_english_letters(text):\n",
    "    pattern = r'[a-zA-Z]'\n",
    "    return re.sub(pattern, '', text)\n",
    "\n",
    "def clean_text(text):\n",
    "    cleaned = re.sub(r'[\\n\\t]+', ' ', text)\n",
    "    cleaned = re.sub(r'\\s+', ' ', cleaned)\n",
    "    return cleaned.strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "def resize_image(image, min_factor=28, max_size=(1024, 1024)):\n",
    "    width, height = image.size\n",
    "    factor = min_factor  # Must be divisible by this (processor's patch size * merge_size)\n",
    "    \n",
    "    # Calculate aspect ratio\n",
    "    aspect_ratio = width / height\n",
    "    \n",
    "    # Ensure dimensions are at least factor and divisible by factor\n",
    "    new_width = max(factor, width)\n",
    "    new_height = max(factor, height)\n",
    "    \n",
    "    # Adjust to be multiples of factor while preserving aspect ratio as closely as possible\n",
    "    new_width = math.ceil(new_width / factor) * factor\n",
    "    new_height = math.ceil(new_height / factor) * factor\n",
    "    \n",
    "    # Recalculate to maintain aspect ratio\n",
    "    if new_width / new_height > aspect_ratio:\n",
    "        new_width = int(new_height * aspect_ratio)\n",
    "        new_width = math.ceil(new_width / factor) * factor  # Ensure divisible by factor\n",
    "    else:\n",
    "        new_height = int(new_width / aspect_ratio)\n",
    "        new_height = math.ceil(new_height / factor) * factor  # Ensure divisible by factor\n",
    "    \n",
    "    # Ensure within max_size\n",
    "    if new_width > max_size[0] or new_height > max_size[1]:\n",
    "        if new_width > max_size[0]:\n",
    "            new_width = max_size[0] - (max_size[0] % factor)  # Largest multiple of factor <= max_size[0]\n",
    "            new_height = int(new_width / aspect_ratio)\n",
    "            new_height = math.ceil(new_height / factor) * factor\n",
    "        if new_height > max_size[1]:\n",
    "            new_height = max_size[1] - (max_size[1] % factor)  # Largest multiple of factor <= max_size[1]\n",
    "            new_width = int(new_height * aspect_ratio)\n",
    "            new_width = math.ceil(new_width / factor) * factor\n",
    "    \n",
    "    # Resize if necessary\n",
    "    if (new_width, new_height) != (width, height):\n",
    "        image = image.resize((new_width, new_height), Image.LANCZOS)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_dataset(dataset_name, split=\"train\"):\n",
    "    print(f\"Loading dataset: {dataset_name}\")\n",
    "    dataset = load_dataset(dataset_name, split=split)\n",
    "    \n",
    "    results = []\n",
    "    sample_keys = dataset[0].keys()  \n",
    "    ground_truth_key = \"text\" if \"text\" in sample_keys else \"answer\" if \"answer\" in sample_keys else None\n",
    "    if ground_truth_key is None:\n",
    "        raise ValueError(f\"No suitable ground truth key ('text' or 'answer') found in dataset: {dataset_name}\")\n",
    "    \n",
    "    # Prepare samples\n",
    "    for sample in tqdm(dataset, desc=f\"Preparing samples from {dataset_name}\"):\n",
    "        image = sample[\"image\"]\n",
    "        if image.mode != \"RGB\":\n",
    "            image = image.convert(\"RGB\")\n",
    "        ground_truth = sample[ground_truth_key]\n",
    "        results.append({\n",
    "            \"dataset\": dataset_name,\n",
    "            \"image\": image,\n",
    "            \"ground_truth\": ground_truth,\n",
    "            \"ain\": None,\n",
    "            \"status\": \"pending\"  # Add status to track processing outcome\n",
    "        })\n",
    "    \n",
    "    skipped_samples = 0\n",
    "    for i, sample in tqdm(enumerate(results), total=len(results), desc=f\"Running ain on {dataset_name}\"):\n",
    "        # torch.cuda.empty_cache()\n",
    "        image = sample[\"image\"]\n",
    "        try:\n",
    "            text_result = qwen_extract_ocr(text_prompt, image)\n",
    "            results[i][\"ain\"] = text_result\n",
    "            results[i][\"status\"] = \"success\"\n",
    "        except RuntimeError as e:\n",
    "            if \"CUDA out of memory\" in str(e):\n",
    "                print(f\"Skipping sample {i} in {dataset_name} due to CUDA out of memory error.\")\n",
    "                results[i][\"ain\"] = \"Skipped - CUDA OOM\"\n",
    "                results[i][\"status\"] = \"skipped\"\n",
    "                skipped_samples += 1\n",
    "            else:\n",
    "                print(f\"Error processing sample {i} in {dataset_name}: {str(e)}\")\n",
    "                results[i][\"ain\"] = f\"Error - {str(e)}\"\n",
    "                results[i][\"status\"] = \"error\"\n",
    "    \n",
    "    print(f\"Processed {len(results) - skipped_samples} samples successfully, skipped {skipped_samples} due to memory issues in {dataset_name}.\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset: ahmedheakl/arocrbench_synthesizear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing samples from ahmedheakl/arocrbench_synthesizear: 100%|██████████| 500/500 [00:00<00:00, 1112.94it/s]\n",
      "Running ain on ahmedheakl/arocrbench_synthesizear: 100%|██████████| 500/500 [18:48<00:00,  2.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 500 samples successfully, skipped 0 due to memory issues in ahmedheakl/arocrbench_synthesizear.\n",
      "Loading dataset: ahmedheakl/arocrbench_patsocr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing samples from ahmedheakl/arocrbench_patsocr: 100%|██████████| 500/500 [00:00<00:00, 683.37it/s]\n",
      "Running ain on ahmedheakl/arocrbench_patsocr: 100%|██████████| 500/500 [13:23<00:00,  1.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 500 samples successfully, skipped 0 due to memory issues in ahmedheakl/arocrbench_patsocr.\n",
      "Loading dataset: ahmedheakl/arocrbench_arabicocr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing samples from ahmedheakl/arocrbench_arabicocr: 100%|██████████| 50/50 [00:00<00:00, 741.43it/s]\n",
      "Running ain on ahmedheakl/arocrbench_arabicocr: 100%|██████████| 50/50 [04:23<00:00,  5.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 50 samples successfully, skipped 0 due to memory issues in ahmedheakl/arocrbench_arabicocr.\n",
      "Loading dataset: ahmedheakl/arocrbench_hindawi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing samples from ahmedheakl/arocrbench_hindawi: 100%|██████████| 200/200 [00:01<00:00, 174.91it/s]\n",
      "Running ain on ahmedheakl/arocrbench_hindawi: 100%|██████████| 200/200 [1:03:44<00:00, 19.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 200 samples successfully, skipped 0 due to memory issues in ahmedheakl/arocrbench_hindawi.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_results = []\n",
    "for dataset_name in datasets_to_benchmark:\n",
    "    dataset_results = benchmark_dataset(dataset_name)\n",
    "    all_results.extend(dataset_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>image</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>ain</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [dataset, image, ground_truth, ain, status]\n",
       "Index: []"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['status']!='success']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.to_csv(\"uncleand_ain.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>image</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>ain</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ahmedheakl/arocrbench_synthesizear</td>\n",
       "      <td>&lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=639x114 at 0x7EA68961B490&gt;</td>\n",
       "      <td>وَإِذا ما سَأَلَتْنِي عَن مَعْنَى لَفَظَهُ \"عرب\" عِنْدَ</td>\n",
       "      <td>وَإِذا ما سَأَلَتْنِي عَن مَعْنَى لَفَظَهُ \"عرب\" عِنْدَ</td>\n",
       "      <td>success</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ahmedheakl/arocrbench_synthesizear</td>\n",
       "      <td>&lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1182x147 at 0x7EA68961B9D0&gt;</td>\n",
       "      <td>أَمّا فَهُم النُصُوصِ وَاِسْتِنْباط مَعانِيها بِوَجْهٍ صَحِيحٌ دقيق،</td>\n",
       "      <td>أَمّا فَهُم النُصُوصِ وَاِسْتِنْباط مَعانِيها بِوَجْهٍ صَحِيحٌ دقيق،</td>\n",
       "      <td>success</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ahmedheakl/arocrbench_synthesizear</td>\n",
       "      <td>&lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=994x163 at 0x7EA68961BED0&gt;</td>\n",
       "      <td>تُثِير فِيها الاسود، وَهُوَ ما يَتَعارَض مَعَ اِكْتِشافِ</td>\n",
       "      <td>تُثِير فِيها الاسود، وَهُوَ ما يَتَعارَض مَعَ اِكْتِشافِ</td>\n",
       "      <td>success</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ahmedheakl/arocrbench_synthesizear</td>\n",
       "      <td>&lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1315x224 at 0x7EA6301D6690&gt;</td>\n",
       "      <td>الجماعي، وَكادَت تَصِل إِلَى المَرْحَلَةِ الاِشْتِراكِيَّة لِسَيْطَرَةِ أَكْبَرَ</td>\n",
       "      <td>الجماعي، وَكادَت تَصِل إِلَى المَرْحَلَةِ الاِشْتِراكِيَّة لِسَيْطَرَةِ أَكْبَرَ</td>\n",
       "      <td>success</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ahmedheakl/arocrbench_synthesizear</td>\n",
       "      <td>&lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1224x129 at 0x7EA689678210&gt;</td>\n",
       "      <td>مَعَهُ مَنْدِيلا فِيهِ جردقتان وَقَطَعَ لَحْم سَكْباج مُبَرَّد</td>\n",
       "      <td>مَعَهُ مَنْدِيلا فِيهِ جردقتان وَقَطَعَ لَحْم سَكْباج مُبَرَّد</td>\n",
       "      <td>success</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              dataset  \\\n",
       "0  ahmedheakl/arocrbench_synthesizear   \n",
       "1  ahmedheakl/arocrbench_synthesizear   \n",
       "2  ahmedheakl/arocrbench_synthesizear   \n",
       "3  ahmedheakl/arocrbench_synthesizear   \n",
       "4  ahmedheakl/arocrbench_synthesizear   \n",
       "\n",
       "                                                                                image  \\\n",
       "0   <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=639x114 at 0x7EA68961B490>   \n",
       "1  <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1182x147 at 0x7EA68961B9D0>   \n",
       "2   <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=994x163 at 0x7EA68961BED0>   \n",
       "3  <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1315x224 at 0x7EA6301D6690>   \n",
       "4  <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1224x129 at 0x7EA689678210>   \n",
       "\n",
       "                                                                       ground_truth  \\\n",
       "0                           وَإِذا ما سَأَلَتْنِي عَن مَعْنَى لَفَظَهُ \"عرب\" عِنْدَ   \n",
       "1              أَمّا فَهُم النُصُوصِ وَاِسْتِنْباط مَعانِيها بِوَجْهٍ صَحِيحٌ دقيق،   \n",
       "2                          تُثِير فِيها الاسود، وَهُوَ ما يَتَعارَض مَعَ اِكْتِشافِ   \n",
       "3  الجماعي، وَكادَت تَصِل إِلَى المَرْحَلَةِ الاِشْتِراكِيَّة لِسَيْطَرَةِ أَكْبَرَ   \n",
       "4                    مَعَهُ مَنْدِيلا فِيهِ جردقتان وَقَطَعَ لَحْم سَكْباج مُبَرَّد   \n",
       "\n",
       "                                                                                ain  \\\n",
       "0                           وَإِذا ما سَأَلَتْنِي عَن مَعْنَى لَفَظَهُ \"عرب\" عِنْدَ   \n",
       "1              أَمّا فَهُم النُصُوصِ وَاِسْتِنْباط مَعانِيها بِوَجْهٍ صَحِيحٌ دقيق،   \n",
       "2                          تُثِير فِيها الاسود، وَهُوَ ما يَتَعارَض مَعَ اِكْتِشافِ   \n",
       "3  الجماعي، وَكادَت تَصِل إِلَى المَرْحَلَةِ الاِشْتِراكِيَّة لِسَيْطَرَةِ أَكْبَرَ   \n",
       "4                    مَعَهُ مَنْدِيلا فِيهِ جردقتان وَقَطَعَ لَحْم سَكْباج مُبَرَّد   \n",
       "\n",
       "    status  \n",
       "0  success  \n",
       "1  success  \n",
       "2  success  \n",
       "3  success  \n",
       "4  success  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"uncleand_ain.csv\",index_col=\"Unnamed: 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"ground_truth\"] = df[\"ground_truth\"].astype(str).fillna(\"\").apply(remove_english_letters).apply(clean_text).astype(str)\n",
    "df[\"ain\"] = df[\"ain\"].astype(str).fillna(\"\").apply(remove_english_letters).apply(clean_text).astype(str)\n",
    "df[\"ground_truth_t\"] = df[\"ground_truth\"].astype(str).fillna(\"\").apply(remove_diacritics).astype(str)\n",
    "df[\"ain_t\"] = df[\"ain\"].astype(str).fillna(\"\").apply(remove_diacritics).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ahmedheakl/arocrbench_synthesizear',\n",
       "       'ahmedheakl/arocrbench_patsocr', 'ahmedheakl/arocrbench_arabicocr',\n",
       "       'ahmedheakl/arocrbench_hindawi'], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['dataset'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for ahmedheakl/arocrbench_synthesizear:\n",
      "ain - WER: 0.17, CER: 0.04\n",
      "ain (no diacritics) - WER: 0.02, CER: 0.00\n",
      "\n",
      "Results for ahmedheakl/arocrbench_patsocr:\n",
      "ain - WER: 0.01, CER: 0.00\n",
      "ain (no diacritics) - WER: 0.01, CER: 0.00\n",
      "\n",
      "Results for ahmedheakl/arocrbench_arabicocr:\n",
      "ain - WER: 0.01, CER: 0.00\n",
      "ain (no diacritics) - WER: 0.01, CER: 0.00\n",
      "\n",
      "Results for ahmedheakl/arocrbench_hindawi:\n",
      "ain - WER: 0.14, CER: 0.07\n",
      "ain (no diacritics) - WER: 0.09, CER: 0.05\n"
     ]
    }
   ],
   "source": [
    "for dataset_name in datasets_to_benchmark:\n",
    "    subset_df = df[df[\"dataset\"] == dataset_name]\n",
    "    print(f\"\\nResults for {dataset_name}:\")\n",
    "    \n",
    "    wer_score = wer(subset_df[\"ground_truth\"].tolist(), subset_df[\"ain\"].tolist())\n",
    "    cer_score = cer(subset_df[\"ground_truth\"].tolist(), subset_df[\"ain\"].tolist())\n",
    "    print(f\"ain - WER: {wer_score:.2f}, CER: {cer_score:.2f}\")\n",
    "    \n",
    "    wer_score_t = wer(subset_df[\"ground_truth_t\"].tolist(), subset_df[\"ain_t\"].tolist())\n",
    "    cer_score_t = cer(subset_df[\"ground_truth_t\"].tolist(), subset_df[\"ain_t\"].tolist())\n",
    "    print(f\"ain (no diacritics) - WER: {wer_score_t:.2f}, CER: {cer_score_t:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to 'ain_ocr_benchmark_results.csv'\n"
     ]
    }
   ],
   "source": [
    "df.to_csv(\"ain_ocr_benchmark_results.csv\", index=False)\n",
    "print(\"Results saved to 'ain_ocr_benchmark_results.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of results:\n",
      "                              dataset  \\\n",
      "0  ahmedheakl/arocrbench_synthesizear   \n",
      "1  ahmedheakl/arocrbench_synthesizear   \n",
      "2  ahmedheakl/arocrbench_synthesizear   \n",
      "3  ahmedheakl/arocrbench_synthesizear   \n",
      "4  ahmedheakl/arocrbench_synthesizear   \n",
      "\n",
      "                                                                       ground_truth  \\\n",
      "0                           وَإِذا ما سَأَلَتْنِي عَن مَعْنَى لَفَظَهُ \"عرب\" عِنْدَ   \n",
      "1              أَمّا فَهُم النُصُوصِ وَاِسْتِنْباط مَعانِيها بِوَجْهٍ صَحِيحٌ دقيق،   \n",
      "2                          تُثِير فِيها الاسود، وَهُوَ ما يَتَعارَض مَعَ اِكْتِشافِ   \n",
      "3  الجماعي، وَكادَت تَصِل إِلَى المَرْحَلَةِ الاِشْتِراكِيَّة لِسَيْطَرَةِ أَكْبَرَ   \n",
      "4                    مَعَهُ مَنْدِيلا فِيهِ جردقتان وَقَطَعَ لَحْم سَكْباج مُبَرَّد   \n",
      "\n",
      "                                                                              qari  \n",
      "0                            وإِذا ما سَأَلْتَني عَن مَعْنِي لَفَظَهُ \"عرب\" عَنْدَ  \n",
      "1                     أما فهُم النُّصوص وَإِسْتَنباط قمانيها بِوجْهِ صَحِيحٍ دقيق،  \n",
      "2                     تَثْيرٍ فيِهاِ الـسود، وَهُوَ ما يَتَعَازِضَ مَعِ إِكْتِشافٍ  \n",
      "3  جماعة، وَكَادَت تَصل إِلَى المَرْحَلةِ الاِشْتِراَكِيَّةِ لِسَيْظَرِهِ أُمْبَرَ  \n",
      "4                        معهُ مَنْديلا فِيهِ جردقتان وَقَطعَ لَحْم سُكْباج مُبَرَد  \n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSample of results:\")\n",
    "print(df[[\"dataset\", \"ground_truth\", \"ain\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
