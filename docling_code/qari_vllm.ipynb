{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Read the VLLm \n",
    "2. Learn how to use it efficiently \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-19 15:58:55 [__init__.py:256] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pdf2image import convert_from_path\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.lora.request import LoRARequest\n",
    "import logging\n",
    "from typing import Dict\n",
    "from huggingface_hub import  snapshot_download\n",
    "\n",
    "# os.environ['HF_TOKEN'] = ''\n",
    "os.environ['HF_HUB_ENABLE_HF_TRANSFER']='1' \n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "_log = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f3d255396b84953a9b6a8351bf11b17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 13 files:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qari_lora_path = snapshot_download(repo_id=\"NAMAA-Space/Qari-OCR-0.2.2-Arabic-2B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def convert_pdf_with_qwen_vllm(pdf_path: str, start_page: int = 1, end_page: int = None) -> Dict[int, str]:\n",
    "    start_idx = start_page - 1\n",
    "    end_idx = end_page if end_page is not None else None\n",
    "\n",
    "    _log.info(f\"Converting PDF {pdf_path} to images for pages {start_page} to {end_page or 'end'}...\")\n",
    "    try:\n",
    "        images = convert_from_path(pdf_path, first_page=start_page, last_page=end_page)\n",
    "    except Exception as e:\n",
    "        _log.error(f\"Failed to convert PDF to images: {e}\")\n",
    "        return {}\n",
    "\n",
    "    if not images:\n",
    "        _log.warning(\"No images extracted from PDF.\")\n",
    "        return {}\n",
    "\n",
    "    model_name = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "    _log.info(f\"Loading model {model_name} with vLLM...\")\n",
    "    try:\n",
    "        llm = LLM(\n",
    "            model=model_name,\n",
    "            enable_lora=True,\n",
    "            dtype=\"float16\",\n",
    "            max_model_len=4096,\n",
    "            # max_num_seqs=1,\n",
    "            disable_mm_preprocessor_cache=False,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        _log.error(f\"Failed to load Qwen2.5-VL model with vLLM: {e}\")\n",
    "        return {}\n",
    "\n",
    "    question = \"Extract all text from this image\"\n",
    "    prompt_template = (\n",
    "        \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n\"\n",
    "        f\"<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>{question}<|im_end|>\\n\"\n",
    "        \"<|im_start|>assistant\\n\"\n",
    "    )\n",
    "\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=0.0,  # Greedy decoding for consistency\n",
    "        max_tokens=4096,   # Adjust based on expected text length\n",
    "        stop_token_ids=None,\n",
    "    )\n",
    "\n",
    "    extracted_texts = {}\n",
    "    total_pages = len(images)\n",
    "    with tqdm(total=total_pages, desc=\"Processing PDF pages\", unit=\"page\") as pbar:\n",
    "        for i, image in enumerate(images, start=start_idx):\n",
    "            page_num = i + 1  # Convert back to 1-based indexing\n",
    "            _log.info(f\"Processing page {page_num}...\")\n",
    "\n",
    "            inputs = {\n",
    "                \"prompt\": prompt_template,\n",
    "                \"multi_modal_data\": {\n",
    "                    \"image\": image  # Pass PIL image directly\n",
    "                },\n",
    "            }\n",
    "\n",
    "            try:\n",
    "                outputs = llm.generate([inputs], sampling_params=sampling_params, lora_request=LoRARequest(\"qari_adapter\", 1, qari_lora_path))\n",
    "                generated_text = outputs[0].outputs[0].text.strip()\n",
    "                extracted_texts[page_num] = generated_text\n",
    "            except Exception as e:\n",
    "                _log.error(f\"Error processing page {page_num}: {e}\")\n",
    "                extracted_texts[page_num] = f\"Error: {str(e)}\"\n",
    "            \n",
    "            pbar.update(1)  # Update progress bar after each page\n",
    "\n",
    "    return extracted_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Converting PDF ../cold_war_data/cold_war.pdf to images for pages 9 to 11...\n",
      "INFO:__main__:Loading model Qwen/Qwen2-VL-2B-Instruct with vLLM...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-19 15:58:58 [config.py:2599] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 03-19 15:59:05 [config.py:583] This model supports multiple tasks: {'generate', 'classify', 'reward', 'embed', 'score'}. Defaulting to 'generate'.\n",
      "WARNING 03-19 15:59:05 [arg_utils.py:1754] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \n",
      "INFO 03-19 15:59:05 [llm_engine.py:241] Initializing a V0 LLM engine (v0.8.0) with config: model='Qwen/Qwen2-VL-2B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2-VL-2B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen2-VL-2B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 03-19 15:59:07 [cuda.py:234] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 03-19 15:59:07 [cuda.py:282] Using XFormers backend.\n",
      "INFO 03-19 15:59:07 [parallel_state.py:967] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 03-19 15:59:07 [model_runner.py:1110] Starting to load model Qwen/Qwen2-VL-2B-Instruct...\n",
      "INFO 03-19 15:59:07 [config.py:3222] cudagraph sizes specified by model runner [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256] is overridden by config [256, 128, 2, 1, 4, 136, 8, 144, 16, 152, 24, 160, 32, 168, 40, 176, 48, 184, 56, 192, 64, 200, 72, 208, 80, 216, 88, 120, 224, 96, 232, 104, 240, 112, 248]\n",
      "INFO 03-19 15:59:08 [weight_utils.py:257] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e4809b3b18246b2b8368b8bf5aa1a1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-19 15:59:09 [loader.py:429] Loading weights took 1.12 seconds\n",
      "WARNING 03-19 15:59:09 [model_runner.py:1120] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "INFO 03-19 15:59:09 [punica_selector.py:18] Using PunicaWrapperGPU.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.patch_embed.proj will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.0 will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.0.attn.qkv will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.0.attn.proj will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.0.mlp.fc1 will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.0.mlp.fc2 will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.1.attn.qkv will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.1.attn.proj will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.1.mlp.fc1 will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.1.mlp.fc2 will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.2 will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.2.attn.qkv will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.2.attn.proj will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.2.mlp.fc1 will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.2.mlp.fc2 will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.3.attn.qkv will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.3.attn.proj will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.3.mlp.fc1 will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.3.mlp.fc2 will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.4.attn.qkv will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.4.attn.proj will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.4.mlp.fc1 will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.4.mlp.fc2 will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.5.attn.qkv will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.5.attn.proj will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.5.mlp.fc1 will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.5.mlp.fc2 will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.6.attn.qkv will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.6.attn.proj will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.6.mlp.fc1 will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.6.mlp.fc2 will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.7.attn.qkv will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.7.attn.proj will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.7.mlp.fc1 will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.7.mlp.fc2 will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.8.attn.qkv will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.8.attn.proj will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.8.mlp.fc1 will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.8.mlp.fc2 will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.9.attn.qkv will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.9.attn.proj will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.9.mlp.fc1 will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.9.mlp.fc2 will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.10.attn.qkv will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.10.attn.proj will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.10.mlp.fc1 will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.10.mlp.fc2 will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.11.attn.qkv will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.11.attn.proj will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.11.mlp.fc1 will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.11.mlp.fc2 will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.12.attn.qkv will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.12.attn.proj will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.12.mlp.fc1 will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.12.mlp.fc2 will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.13.attn.qkv will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.13.attn.proj will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.13.mlp.fc1 will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.13.mlp.fc2 will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.14.attn.qkv will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.14.attn.proj will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.14.mlp.fc1 will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.14.mlp.fc2 will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.15.attn.qkv will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.15.attn.proj will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.15.mlp.fc1 will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.15.mlp.fc2 will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.16.attn.qkv will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.16.attn.proj will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.16.mlp.fc1 will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.16.mlp.fc2 will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.17.attn.qkv will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.17.attn.proj will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.17.mlp.fc1 will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.17.mlp.fc2 will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.18.attn.qkv will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.18.attn.proj will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.18.mlp.fc1 will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.18.mlp.fc2 will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.19.attn.qkv will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.19.attn.proj will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.19.mlp.fc1 will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.19.mlp.fc2 will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.20.attn.qkv will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.20.attn.proj will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.20.mlp.fc1 will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.20.mlp.fc2 will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.21.attn.qkv will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.21.attn.proj will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.21.mlp.fc1 will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.21.mlp.fc2 will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.22.attn.qkv will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.22.attn.proj will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.22.mlp.fc1 will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.22.mlp.fc2 will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.23.attn.qkv will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.23.attn.proj will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.23.mlp.fc1 will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.23.mlp.fc2 will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.24.attn.qkv will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.24.attn.proj will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.24.mlp.fc1 will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.24.mlp.fc2 will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.25.attn.qkv will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.25.attn.proj will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.25.mlp.fc1 will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.25.mlp.fc2 will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.26.attn.qkv will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.26.attn.proj will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.26.mlp.fc1 will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.26.mlp.fc2 will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.27.attn.qkv will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.27.attn.proj will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.27.mlp.fc1 will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.27.mlp.fc2 will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.28.attn.qkv will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.28.attn.proj will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.28.mlp.fc1 will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.28.mlp.fc2 will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.29.attn.qkv will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.29.attn.proj will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.29.mlp.fc1 will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.29.mlp.fc2 will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.30.attn.qkv will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.30.attn.proj will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.30.mlp.fc1 will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.30.mlp.fc2 will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.31.attn.qkv will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.31.attn.proj will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.31.mlp.fc1 will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.31.mlp.fc2 will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.merger.mlp.0 will be ignored.\n",
      "WARNING 03-19 15:59:09 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.merger.mlp.2 will be ignored.\n",
      "INFO 03-19 15:59:10 [model_runner.py:1146] Model loading took 4.1859 GB and 2.230534 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-19 15:59:11 [model_runner.py:1296] Computed max_num_seqs (min(256, 5120 // 32768)) to be less than 1. Setting it to the minimum value of 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-19 15:59:16 [profiling.py:222] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 5120) is too short to hold the multi-modal embeddings in the worst case (32768 tokens in total, out of which {'image': 16384, 'video': 16384} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "INFO 03-19 15:59:18 [worker.py:267] Memory profiling takes 8.31 seconds\n",
      "INFO 03-19 15:59:18 [worker.py:267] the current vLLM instance can use total_gpu_memory (23.64GiB) x gpu_memory_utilization (0.90) = 21.27GiB\n",
      "INFO 03-19 15:59:18 [worker.py:267] model weights take 4.19GiB; non_torch_memory takes 0.06GiB; PyTorch activation peak memory takes 0.33GiB; the rest of the memory reserved for KV Cache is 16.70GiB.\n",
      "INFO 03-19 15:59:19 [executor_base.py:111] # cuda blocks: 39096, # CPU blocks: 9362\n",
      "INFO 03-19 15:59:19 [executor_base.py:116] Maximum concurrency for 4096 tokens per request: 152.72x\n",
      "INFO 03-19 15:59:21 [model_runner.py:1442] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:24<00:00,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-19 15:59:45 [model_runner.py:1570] Graph capturing finished in 24 secs, took 0.41 GiB\n",
      "INFO 03-19 15:59:45 [llm_engine.py:447] init engine (profile, create kv cache, warmup model) took 35.72 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing PDF pages:   0%|          | 0/3 [00:00<?, ?page/s]INFO:__main__:Processing page 9...\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.65s/it, est. speed input: 1655.63 toks/s, output: 11.54 toks/s]\n",
      "Processing PDF pages:  33%|███▎      | 1/3 [00:04<00:09,  4.57s/page]INFO:__main__:Processing page 10...\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.29s/it, est. speed input: 2122.26 toks/s, output: 5.45 toks/s]\n",
      "Processing PDF pages:  67%|██████▋   | 2/3 [00:06<00:02,  2.76s/page]INFO:__main__:Processing page 11...\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.51s/it, est. speed input: 1806.82 toks/s, output: 19.21 toks/s]\n",
      "Processing PDF pages: 100%|██████████| 3/3 [00:07<00:00,  2.59s/page]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 9 Text:\n",
      "مقدمة الأعمال الكاملة للكاتب والمترجم طلعت الشايب\n",
      "--------------------------------------------------\n",
      "Page 10 Text:\n",
      "أدب الحرب الباردة\n",
      "--------------------------------------------------\n",
      "Page 11 Text:\n",
      "مقدّمة المحرّر بين الخطاب والرد: (أفكار تمهيدية عن كتابة الحرب الباردة)\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pdf_file = \"../cold_war_data/cold_war.pdf\"  # Replace with your PDF path\n",
    "start = 9\n",
    "end = 11\n",
    "\n",
    "result = convert_pdf_with_qwen_vllm(pdf_file, start_page=start, end_page=end)\n",
    "\n",
    "# Print results\n",
    "for page_num, text in result.items():\n",
    "    print(f\"Page {page_num} Text:\")\n",
    "    print(f\"{text}\")\n",
    "    print('-' * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
