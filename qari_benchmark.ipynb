{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "from jiwer import wer, cer\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_to_benchmark = [    \n",
    "    \"ahmedheakl/arocrbench_synthesizear\",\n",
    "    \"ahmedheakl/arocrbench_patsocr\",\n",
    "    \"ahmedheakl/arocrbench_historyar\",\n",
    "    \"ahmedheakl/arocrbench_historicalbooks\",\n",
    "    \"ahmedheakl/arocrbench_khattparagraph\",\n",
    "    \"ahmedheakl/arocrbench_adab\",\n",
    "    \"ahmedheakl/arocrbench_muharaf\",\n",
    "    \"ahmedheakl/arocrbench_onlinekhatt\",\n",
    "    \"ahmedheakl/arocrbench_khatt\",\n",
    "    \"ahmedheakl/arocrbench_isippt\",\n",
    "    \"ahmedheakl/arocrbench_arabicocr\",\n",
    "    \"ahmedheakl/arocrbench_hindawi\",\n",
    "    \"ahmedheakl/arocrbench_evarest\"\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"NAMAA-Space/Qari-OCR-0.1-VL-2B-Instruct\", use_fast=True)\n",
    "model = AutoModelForImageTextToText.from_pretrained(\"NAMAA-Space/Qari-OCR-0.1-VL-2B-Instruct\", torch_dtype=\"auto\", device_map=\"cuda:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": \"Below is the image of one page of a document. Just return the plain text representation of this document as if you were reading it naturally. Do not hallucinate\"}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "text_prompt = processor.apply_chat_template(messages, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qwen_extract_ocr(text_prompt, image):\n",
    "    inputs = processor(text=[text_prompt], images=[image], padding=True, return_tensors=\"pt\")\n",
    "    inputs = inputs.to(\"cuda:1\")\n",
    "    output_ids = model.generate(**inputs, max_new_tokens=1024)\n",
    "    generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]\n",
    "    output_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    return \" \".join(output_text).replace(\"The Arabic text in the image is:\\n\\n\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_diacritics(text):\n",
    "    diacritics = [\n",
    "        '\\u0617', '\\u0618', '\\u0619', '\\u061A',\n",
    "        '\\u064B', '\\u064C', '\\u064D', '\\u064E', '\\u064F', '\\u0650',\n",
    "        '\\u0651', '\\u0652', '\\u0653', '\\u0654', '\\u0655', '\\u0656',\n",
    "        '\\u0657', '\\u0658', '\\u0659', '\\u065A', '\\u065B', '\\u065C',\n",
    "        '\\u065D', '\\u065E', '\\u065F', '\\u0670'\n",
    "    ]\n",
    "    pattern = '[' + ''.join(diacritics) + ']'\n",
    "    return re.sub(pattern, '', text)\n",
    "\n",
    "def remove_english_letters(text):\n",
    "    pattern = r'[a-zA-Z]'\n",
    "    return re.sub(pattern, '', text)\n",
    "\n",
    "def clean_text(text):\n",
    "    cleaned = re.sub(r'[\\n\\t]+', ' ', text)\n",
    "    cleaned = re.sub(r'\\s+', ' ', cleaned)\n",
    "    return cleaned.strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_dataset(dataset_name, split=\"train\"):\n",
    "    print(f\"Loading dataset: {dataset_name}\")\n",
    "    dataset = load_dataset(dataset_name, split=split)\n",
    "    \n",
    "    results = []\n",
    "    sample_keys = dataset[0].keys()  \n",
    "    ground_truth_key = \"text\" if \"text\" in sample_keys else \"answer\" if \"answer\" in sample_keys else None\n",
    "    if ground_truth_key is None:\n",
    "        raise ValueError(f\"No suitable ground truth key ('text' or 'answer') found in dataset: {dataset_name}\")\n",
    "    \n",
    "    # Prepare samples\n",
    "    for sample in tqdm(dataset, desc=f\"Preparing samples from {dataset_name}\"):\n",
    "        image = sample[\"image\"]\n",
    "        if image.mode != \"RGB\":\n",
    "            image = image.convert(\"RGB\")\n",
    "        ground_truth = sample[ground_truth_key]\n",
    "        results.append({\n",
    "            \"dataset\": dataset_name,\n",
    "            \"image\": image,\n",
    "            \"ground_truth\": ground_truth,\n",
    "            \"qari\": None,\n",
    "            \"status\": \"pending\"  # Add status to track processing outcome\n",
    "        })\n",
    "    \n",
    "    skipped_samples = 0\n",
    "    for i, sample in tqdm(enumerate(results), total=len(results), desc=f\"Running Qari on {dataset_name}\"):\n",
    "        torch.cuda.empty_cache()\n",
    "        image = sample[\"image\"]\n",
    "        try:\n",
    "            text_result = qwen_extract_ocr(text_prompt, image)\n",
    "            results[i][\"qari\"] = text_result\n",
    "            results[i][\"status\"] = \"success\"\n",
    "        except RuntimeError as e:\n",
    "            if \"CUDA out of memory\" in str(e):\n",
    "                print(f\"Skipping sample {i} in {dataset_name} due to CUDA out of memory error.\")\n",
    "                results[i][\"qari\"] = \"Skipped - CUDA OOM\"\n",
    "                results[i][\"status\"] = \"skipped\"\n",
    "                skipped_samples += 1\n",
    "            else:\n",
    "                print(f\"Error processing sample {i} in {dataset_name}: {str(e)}\")\n",
    "                results[i][\"qari\"] = f\"Error - {str(e)}\"\n",
    "                results[i][\"status\"] = \"error\"\n",
    "    \n",
    "    print(f\"Processed {len(results) - skipped_samples} samples successfully, skipped {skipped_samples} due to memory issues in {dataset_name}.\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset: ahmedheakl/arocrbench_synthesizear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing samples from ahmedheakl/arocrbench_synthesizear: 100%|██████████| 500/500 [00:00<00:00, 1135.05it/s]\n",
      "Running Qari on ahmedheakl/arocrbench_synthesizear:   3%|▎         | 16/500 [01:00<39:10,  4.86s/it]"
     ]
    }
   ],
   "source": [
    "all_results = []\n",
    "for dataset_name in datasets_to_benchmark:\n",
    "    dataset_results = benchmark_dataset(dataset_name)\n",
    "    all_results.extend(dataset_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"uncleand_qari.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"ground_truth\"] = df[\"ground_truth\"].apply(remove_english_letters).apply(clean_text)\n",
    "df[\"qari\"] = df[\"qari\"].apply(remove_english_letters).apply(clean_text)\n",
    "df[\"ground_truth_t\"] = df[\"ground_truth\"].apply(remove_diacritics)\n",
    "df[\"qari_t\"] = df[\"qari\"].apply(remove_diacritics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_name in datasets_to_benchmark:\n",
    "    subset_df = df[df[\"dataset\"] == dataset_name]\n",
    "    print(f\"\\nResults for {dataset_name}:\")\n",
    "    \n",
    "    wer_score = wer(subset_df[\"ground_truth\"].tolist(), subset_df[\"qari\"].tolist())\n",
    "    cer_score = cer(subset_df[\"ground_truth\"].tolist(), subset_df[\"qari\"].tolist())\n",
    "    print(f\"Qari - WER: {wer_score:.2f}, CER: {cer_score:.2f}\")\n",
    "    \n",
    "    wer_score_t = wer(subset_df[\"ground_truth_t\"].tolist(), subset_df[\"qari_t\"].tolist())\n",
    "    cer_score_t = cer(subset_df[\"ground_truth_t\"].tolist(), subset_df[\"qari_t\"].tolist())\n",
    "    print(f\"Qari (no diacritics) - WER: {wer_score_t:.2f}, CER: {cer_score_t:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"qari_ocr_benchmark_results.csv\", index=False)\n",
    "print(\"Results saved to 'qari_ocr_benchmark_results.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSample of results:\")\n",
    "print(df[[\"dataset\", \"ground_truth\", \"qari\"]].head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
